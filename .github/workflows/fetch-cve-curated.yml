name: Fetch CVE Curated Data (Hourly)

permissions:
  contents: write

on:
  schedule:
    - cron: '0 * * * *'
  workflow_dispatch:

jobs:
  fetch-cve-curated:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4

      - name: Fetch CVE list + details
        run: |
          mkdir -p cve-curated
          python <<'PY'
          import json
          import os
          import re
          from urllib.parse import urljoin

          import requests
          from bs4 import BeautifulSoup

          BASE = "https://cve.akaoma.com"
          LIST_URL = f"{BASE}/latest-cves"
          OUT_DIR = "cve-curated"

          session = requests.Session()
          session.headers.update({
              "User-Agent": "soc101-support-cve-fetcher/1.0 (+https://github.com)"
          })

          def clean_text(value):
              return re.sub(r"\s+", " ", value or "").strip()

          def find_first_text(soup, labels):
              text = soup.get_text("\n", strip=True)
              for label in labels:
                  m = re.search(rf"{re.escape(label)}\s*[:\-]\s*(.+)", text, re.IGNORECASE)
                  if m:
                      return clean_text(m.group(1))
              return None

          def extract_cvss_vector(soup):
              text = soup.get_text(" ", strip=True)
              m = re.search(r"(CVSS:[0-9]\.[0-9]/[A-Z]{1,4}:[A-Z0-9]+(?:/[A-Z]{1,4}:[A-Z0-9]+)+)", text)
              return m.group(1) if m else None

          def extract_cvss_score(soup):
              text = soup.get_text(" ", strip=True)
              m = re.search(r"\b([0-9]{1,2}(?:\.[0-9])?)\s*/\s*10\b", text)
              if m:
                  try:
                      return float(m.group(1))
                  except ValueError:
                      return None
              return None

          def parse_detail(url):
              r = session.get(url, timeout=30)
              r.raise_for_status()
              soup = BeautifulSoup(r.text, "html.parser")

              page_text = soup.get_text("\n", strip=True)
              cve_id_match = re.search(r"\bCVE-\d{4}-\d{4,7}\b", page_text)
              cve_id = cve_id_match.group(0) if cve_id_match else os.path.basename(url).upper()

              paragraphs = [clean_text(p.get_text(" ", strip=True)) for p in soup.find_all("p")]
              summary = None
              for p in paragraphs:
                  if len(p) > 60 and "CVE-" not in p[:30]:
                      summary = p
                      break

              references = []
              for a in soup.find_all("a", href=True):
                  href = a["href"].strip()
                  if href.startswith("http") and "akaoma.com" not in href:
                      references.append(href)
              references = sorted(set(references))

              data = {
                  "cve_id": cve_id,
                  "status": find_first_text(soup, ["Status"]),
                  "published_at": find_first_text(soup, ["Published", "Date Published"]),
                  "updated_at": find_first_text(soup, ["Updated", "Last Updated", "Date Updated"]),
                  "cvss": {
                      "version": find_first_text(soup, ["CVSS Version"]),
                      "score": extract_cvss_score(soup),
                      "vector": extract_cvss_vector(soup),
                  },
                  "summary": summary,
                  "references": references,
                  "source_url": url,
              }
              return data

          res = session.get(LIST_URL, timeout=30)
          res.raise_for_status()
          soup = BeautifulSoup(res.text, "html.parser")

          detail_urls = []
          for a in soup.find_all("a", href=True):
              href = a["href"].strip()
              if re.match(r"^/cve-\d{4}-\d{4,7}$", href):
                  detail_urls.append(urljoin(BASE, href))
          detail_urls = sorted(set(detail_urls))

          # Safety cap to keep workflow stable. Increase if you want broader coverage.
          detail_urls = detail_urls[:150]

          details = []
          failed = []
          for url in detail_urls:
              try:
                  details.append(parse_detail(url))
              except Exception as e:
                  failed.append({"url": url, "error": str(e)})

          details.sort(key=lambda x: x.get("cve_id") or "")

          with open(os.path.join(OUT_DIR, "cves.json"), "w", encoding="utf-8") as f:
              json.dump(details, f, ensure_ascii=False, indent=2)

          with open(os.path.join(OUT_DIR, "index.json"), "w", encoding="utf-8") as f:
              json.dump(
                  {
                      "source": LIST_URL,
                      "total_fetched": len(details),
                      "total_failed": len(failed),
                      "detail_urls": detail_urls,
                  },
                  f,
                  ensure_ascii=False,
                  indent=2,
              )

          with open(os.path.join(OUT_DIR, "errors.json"), "w", encoding="utf-8") as f:
              json.dump(failed, f, ensure_ascii=False, indent=2)
          PY

      - name: Commit and push if changed
        run: |
          git config user.name "github-actions"
          git config user.email "actions@github.com"
          git add cve-curated/
          git diff --cached --quiet || git commit -m "Auto-update cve-curated data"
          git push
