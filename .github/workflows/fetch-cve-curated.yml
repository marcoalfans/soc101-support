name: Fetch CVE Curated Data (Hourly)

permissions:
  contents: write

on:
  schedule:
    - cron: '30 */2 * * *'
  workflow_dispatch:

jobs:
  fetch-cve-curated:
    runs-on: ubuntu-latest
    env:
      MAX_PAGES: "10000"
      MAX_CVE_DETAILS: "50000"

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4

      - name: Fetch CVE curated data
        run: |
          mkdir -p cve-curated
          python <<'PY'
          import json
          import os
          import re
          from collections import deque
          from urllib.parse import urljoin, urlparse

          import requests
          from bs4 import BeautifulSoup

          BASE = "https://cve.akaoma.com"
          OUT_DIR = "cve-curated"
          MAX_PAGES = int(os.environ.get("MAX_PAGES", "10000"))
          MAX_CVE_DETAILS = int(os.environ.get("MAX_CVE_DETAILS", "50000"))

          SEED_URLS = [
              f"{BASE}/latest-cves",
              f"{BASE}/recently-updated-cve",
              f"{BASE}/top-critical-cve",
              f"{BASE}/cwe/",
              f"{BASE}/capec",
              f"{BASE}/vendor",
              f"{BASE}/cve-archive",
          ]

          RE_CVE_PATH = re.compile(r"^/cve-\d{4}-\d{4,7}/?$")
          RE_CVE_ID = re.compile(r"\bCVE-\d{4}-\d{4,7}\b")
          RE_CVSS_VECTOR = re.compile(r"(CVSS:[0-9]\.[0-9]/[A-Z]{1,4}:[A-Z0-9]+(?:/[A-Z]{1,4}:[A-Z0-9]+)+)")
          RE_CVSS_SCORE = re.compile(r"\b([0-9]{1,2}(?:\.[0-9])?)\s*/\s*10\b")

          # Relevant internal sections to crawl for discovering CVEs and taxonomy pages.
          RELEVANT_PREFIXES = (
              "/latest-cves",
              "/recently-updated-cve",
              "/top-critical-cve",
              "/cwe",
              "/capec",
              "/vendor",
              "/cve-archive",
          )

          session = requests.Session()
          session.headers.update(
              {
                  "User-Agent": "soc101-support-cve-curated-fetcher/1.0 (+https://github.com)",
                  "Accept": "text/html,application/xhtml+xml",
              }
          )

          def clean_text(value):
              return re.sub(r"\s+", " ", value or "").strip()

          def normalize_url(url):
              parsed = urlparse(url)
              if not parsed.scheme:
                  url = urljoin(BASE, url)
                  parsed = urlparse(url)
              if parsed.netloc != "cve.akaoma.com":
                  return None
              path = parsed.path or "/"
              normalized = f"{BASE}{path}"
              return normalized

          def is_relevant_page(path):
              if RE_CVE_PATH.match(path):
                  return True
              for prefix in RELEVANT_PREFIXES:
                  if path == prefix or path.startswith(prefix + "/"):
                      return True
              return False

          def find_first_text(soup, labels):
              text = soup.get_text("\n", strip=True)
              for label in labels:
                  match = re.search(rf"{re.escape(label)}\s*[:\-]\s*(.+)", text, re.IGNORECASE)
                  if match:
                      return clean_text(match.group(1))
              return None

          def extract_cve_urls_and_links(soup):
              cve_urls = set()
              crawl_links = set()
              cwe_links = set()
              capec_links = set()
              vendor_links = set()

              for a in soup.find_all("a", href=True):
                  href = a["href"].strip()
                  text = clean_text(a.get_text(" ", strip=True))
                  nurl = normalize_url(href)
                  if not nurl:
                      continue

                  path = urlparse(nurl).path

                  if RE_CVE_PATH.match(path):
                      cve_urls.add(nurl.rstrip("/"))

                  if is_relevant_page(path):
                      crawl_links.add(nurl)

                  if path.startswith("/cwe"):
                      cwe_links.add((text, nurl))
                  if path.startswith("/capec"):
                      capec_links.add((text, nurl))
                  if path.startswith("/vendor"):
                      vendor_links.add((text, nurl))

              return cve_urls, crawl_links, cwe_links, capec_links, vendor_links

          def parse_detail(url):
              response = session.get(url, timeout=30)
              response.raise_for_status()
              soup = BeautifulSoup(response.text, "html.parser")
              page_text = soup.get_text("\n", strip=True)

              cve_id_match = RE_CVE_ID.search(page_text)
              cve_id = cve_id_match.group(0) if cve_id_match else os.path.basename(url).upper()

              paragraphs = [clean_text(p.get_text(" ", strip=True)) for p in soup.find_all("p")]
              summary = None
              for p in paragraphs:
                  if len(p) >= 40 and "CVE-" not in p[:20]:
                      summary = p
                      break

              references = []
              for a in soup.find_all("a", href=True):
                  href = a["href"].strip()
                  if href.startswith("http") and "akaoma.com" not in href:
                      references.append(href)

              cvss_vector = None
              cvss_match = RE_CVSS_VECTOR.search(page_text.replace("\n", " "))
              if cvss_match:
                  cvss_vector = cvss_match.group(1)

              cvss_score = None
              score_match = RE_CVSS_SCORE.search(page_text.replace("\n", " "))
              if score_match:
                  try:
                      cvss_score = float(score_match.group(1))
                  except ValueError:
                      cvss_score = None

              # Try to capture related taxonomy links from detail page as well.
              cwe_related = sorted(
                  {
                      urljoin(BASE, a["href"].strip())
                      for a in soup.find_all("a", href=True)
                      if normalize_url(a["href"]) and urlparse(normalize_url(a["href"]).rstrip("/")).path.startswith("/cwe")
                  }
              )
              capec_related = sorted(
                  {
                      urljoin(BASE, a["href"].strip())
                      for a in soup.find_all("a", href=True)
                      if normalize_url(a["href"]) and urlparse(normalize_url(a["href"]).rstrip("/")).path.startswith("/capec")
                  }
              )
              vendor_related = sorted(
                  {
                      urljoin(BASE, a["href"].strip())
                      for a in soup.find_all("a", href=True)
                      if normalize_url(a["href"]) and urlparse(normalize_url(a["href"]).rstrip("/")).path.startswith("/vendor")
                  }
              )

              return {
                  "cve_id": cve_id,
                  "status": find_first_text(soup, ["Status"]),
                  "published_at": find_first_text(soup, ["Published", "Date Published"]),
                  "updated_at": find_first_text(soup, ["Updated", "Last Updated", "Date Updated"]),
                  "cvss": {
                      "version": find_first_text(soup, ["CVSS Version"]),
                      "score": cvss_score,
                      "vector": cvss_vector,
                  },
                  "summary": summary,
                  "references": sorted(set(references)),
                  "related": {
                      "cwe_urls": cwe_related,
                      "capec_urls": capec_related,
                      "vendor_urls": vendor_related,
                  },
                  "source_url": url,
              }

          visited_pages = set()
          page_errors = []
          discovered_cve_urls = set()
          discovered_cwe = set()
          discovered_capec = set()
          discovered_vendor = set()

          queue = deque(sorted(set(SEED_URLS)))

          while queue and len(visited_pages) < MAX_PAGES:
              current = queue.popleft()
              if current in visited_pages:
                  continue

              visited_pages.add(current)

              try:
                  res = session.get(current, timeout=30)
                  res.raise_for_status()
                  soup = BeautifulSoup(res.text, "html.parser")
              except Exception as exc:
                  page_errors.append({"url": current, "error": str(exc)})
                  continue

              cve_urls, crawl_links, cwe_links, capec_links, vendor_links = extract_cve_urls_and_links(soup)
              discovered_cve_urls.update(cve_urls)

              for item in cwe_links:
                  discovered_cwe.add(item)
              for item in capec_links:
                  discovered_capec.add(item)
              for item in vendor_links:
                  discovered_vendor.add(item)

              for link in sorted(crawl_links):
                  if link not in visited_pages:
                      queue.append(link)

          cve_detail_urls = sorted(discovered_cve_urls)
          if len(cve_detail_urls) > MAX_CVE_DETAILS:
              cve_detail_urls = cve_detail_urls[:MAX_CVE_DETAILS]

          cve_details = []
          detail_errors = []
          for url in cve_detail_urls:
              try:
                  cve_details.append(parse_detail(url))
              except Exception as exc:
                  detail_errors.append({"url": url, "error": str(exc)})

          cve_details.sort(key=lambda item: item.get("cve_id") or "")

          cwe_json = [
              {"name": name, "url": url}
              for name, url in sorted(discovered_cwe, key=lambda x: (x[0], x[1]))
          ]
          capec_json = [
              {"name": name, "url": url}
              for name, url in sorted(discovered_capec, key=lambda x: (x[0], x[1]))
          ]
          vendor_json = [
              {"name": name, "url": url}
              for name, url in sorted(discovered_vendor, key=lambda x: (x[0], x[1]))
          ]

          with open(os.path.join(OUT_DIR, "cves.json"), "w", encoding="utf-8") as f:
              json.dump(cve_details, f, ensure_ascii=False, indent=2, sort_keys=True)

          with open(os.path.join(OUT_DIR, "cwe.json"), "w", encoding="utf-8") as f:
              json.dump(cwe_json, f, ensure_ascii=False, indent=2, sort_keys=True)

          with open(os.path.join(OUT_DIR, "capec.json"), "w", encoding="utf-8") as f:
              json.dump(capec_json, f, ensure_ascii=False, indent=2, sort_keys=True)

          with open(os.path.join(OUT_DIR, "vendor.json"), "w", encoding="utf-8") as f:
              json.dump(vendor_json, f, ensure_ascii=False, indent=2, sort_keys=True)

          with open(os.path.join(OUT_DIR, "crawl-errors.json"), "w", encoding="utf-8") as f:
              json.dump(sorted(page_errors, key=lambda x: x["url"]), f, ensure_ascii=False, indent=2, sort_keys=True)

          with open(os.path.join(OUT_DIR, "detail-errors.json"), "w", encoding="utf-8") as f:
              json.dump(sorted(detail_errors, key=lambda x: x["url"]), f, ensure_ascii=False, indent=2, sort_keys=True)

          with open(os.path.join(OUT_DIR, "index.json"), "w", encoding="utf-8") as f:
              json.dump(
                  {
                      "seed_urls": sorted(SEED_URLS),
                      "limits": {
                          "max_pages": MAX_PAGES,
                          "max_cve_details": MAX_CVE_DETAILS,
                      },
                      "totals": {
                          "pages_crawled": len(visited_pages),
                          "cve_urls_discovered": len(discovered_cve_urls),
                          "cve_details_fetched": len(cve_details),
                          "cwe_links_discovered": len(cwe_json),
                          "capec_links_discovered": len(capec_json),
                          "vendor_links_discovered": len(vendor_json),
                          "crawl_errors": len(page_errors),
                          "detail_errors": len(detail_errors),
                      },
                      "pages_crawled": sorted(visited_pages),
                  },
                  f,
                  ensure_ascii=False,
                  indent=2,
                  sort_keys=True,
              )
          PY

      - name: Commit and push if changed
        run: |
          git config user.name "github-actions"
          git config user.email "actions@github.com"
          git add cve-curated/
          git diff --cached --quiet || git commit -m "Auto-update cve-curated data"
          git push
