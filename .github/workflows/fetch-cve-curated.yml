name: Fetch CVE Curated Data (Incremental, Every 2 Hours)

permissions:
  contents: write

on:
  schedule:
    - cron: '30 */2 * * *'
  workflow_dispatch:

jobs:
  fetch-cve-curated:
    runs-on: ubuntu-latest
    timeout-minutes: 35
    env:
      MAX_DISCOVERY_PAGES: "1200"
      MAX_DETAIL_FETCH_PER_RUN: "800"
      REQUEST_TIMEOUT_SECONDS: "20"

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4

      - name: Incremental fetch CVE curated data
        run: |
          mkdir -p cve-curated
          python <<'PY'
          import json
          import os
          import re
          from collections import deque
          from urllib.parse import urljoin, urlparse

          import requests
          from bs4 import BeautifulSoup

          BASE = "https://cve.akaoma.com"
          OUT_DIR = "cve-curated"
          MAX_DISCOVERY_PAGES = int(os.environ.get("MAX_DISCOVERY_PAGES", "1200"))
          MAX_DETAIL_FETCH_PER_RUN = int(os.environ.get("MAX_DETAIL_FETCH_PER_RUN", "800"))
          REQUEST_TIMEOUT_SECONDS = int(os.environ.get("REQUEST_TIMEOUT_SECONDS", "20"))

          PRIORITY_SEEDS = [
              f"{BASE}/latest-cves",
              f"{BASE}/recently-updated-cve",
              f"{BASE}/top-critical-cve",
          ]
          DISCOVERY_SEEDS = PRIORITY_SEEDS + [
              f"{BASE}/cwe/",
              f"{BASE}/capec",
              f"{BASE}/vendor",
              f"{BASE}/cve-archive",
          ]

          RE_CVE_PATH = re.compile(r"^/cve-\d{4}-\d{4,7}/?$")
          RE_CVE_ID = re.compile(r"\bCVE-\d{4}-\d{4,7}\b")
          RE_CVSS_VECTOR = re.compile(r"(CVSS:[0-9]\.[0-9]/[A-Z]{1,4}:[A-Z0-9]+(?:/[A-Z]{1,4}:[A-Z0-9]+)+)")
          RE_CVSS_SCORE = re.compile(r"\b([0-9]{1,2}(?:\.[0-9])?)\s*/\s*10\b")

          RELEVANT_PREFIXES = (
              "/latest-cves",
              "/recently-updated-cve",
              "/top-critical-cve",
              "/cwe",
              "/capec",
              "/vendor",
              "/cve-archive",
          )

          session = requests.Session()
          session.headers.update(
              {
                  "User-Agent": "soc101-support-cve-curated-fetcher/2.0 (+https://github.com)",
                  "Accept": "text/html,application/xhtml+xml",
              }
          )

          def clean_text(value):
              return re.sub(r"\s+", " ", value or "").strip()

          def read_json_list(path):
              try:
                  with open(path, "r", encoding="utf-8") as f:
                      data = json.load(f)
                      return data if isinstance(data, list) else []
              except Exception:
                  return []

          def normalize_url(url):
              parsed = urlparse(url)
              if not parsed.scheme:
                  url = urljoin(BASE, url)
                  parsed = urlparse(url)
              if parsed.netloc != "cve.akaoma.com":
                  return None
              path = parsed.path or "/"
              return f"{BASE}{path}".rstrip("/")

          def is_relevant_non_detail_page(path):
              if RE_CVE_PATH.match(path):
                  return False
              for prefix in RELEVANT_PREFIXES:
                  if path == prefix or path.startswith(prefix + "/"):
                      return True
              return False

          def extract_labeled_value(text, labels):
              lines = [clean_text(line) for line in text.splitlines() if clean_text(line)]
              for i, line in enumerate(lines):
                  for label in labels:
                      pattern = rf"^{re.escape(label)}\s*[:\-]?\s*(.*)$"
                      m = re.match(pattern, line, re.IGNORECASE)
                      if not m:
                          continue
                      value = clean_text(m.group(1))
                      if not value and i + 1 < len(lines):
                          value = lines[i + 1]
                      value = re.sub(r"^[^\w]+", "", value).strip()
                      if not value or re.fullmatch(r"[\W_]+", value):
                          return None
                      return value
              return None

          def pick_best_summary(container, cve_id):
              noise_markers = (
                  "CVE Threat Dashboard",
                  "Latest CVEs",
                  "Top Critical CVE",
                  "CVE Updated",
                  "CVE Archive",
              )
              candidates = [clean_text(p.get_text(" ", strip=True)) for p in container.find_all("p")]
              for p in candidates:
                  if len(p) < 60:
                      continue
                  if any(marker in p for marker in noise_markers):
                      continue
                  if p.lower().startswith(("published", "updated", "status", "cvss")):
                      continue
                  if cve_id and cve_id in p:
                      return p
                  if "vulnerability" in p.lower() or "allows" in p.lower() or "affect" in p.lower():
                      return p
              for p in candidates:
                  if len(p) >= 80 and not any(marker in p for marker in noise_markers):
                      return p
              meta = container.find("meta", attrs={"name": "description"})
              if meta and meta.get("content"):
                  meta_text = clean_text(meta.get("content"))
                  if not any(marker in meta_text for marker in noise_markers):
                      return meta_text
              return None

          def extract_links(soup):
              cve_urls = set()
              crawl_links = set()
              cwe_links = set()
              capec_links = set()
              vendor_links = set()

              for a in soup.find_all("a", href=True):
                  href = a["href"].strip()
                  text = clean_text(a.get_text(" ", strip=True))
                  nurl = normalize_url(href)
                  if not nurl:
                      continue

                  path = urlparse(nurl).path

                  if RE_CVE_PATH.match(path):
                      cve_urls.add(nurl)

                  if is_relevant_non_detail_page(path):
                      crawl_links.add(nurl)

                  if path.startswith("/cwe"):
                      cwe_links.add((text, nurl))
                  if path.startswith("/capec"):
                      capec_links.add((text, nurl))
                  if path.startswith("/vendor"):
                      vendor_links.add((text, nurl))

              return cve_urls, crawl_links, cwe_links, capec_links, vendor_links

          def parse_detail(url):
              r = session.get(url, timeout=REQUEST_TIMEOUT_SECONDS)
              r.raise_for_status()
              soup = BeautifulSoup(r.text, "html.parser")
              page_text = soup.get_text("\n", strip=True)

              cve_match = RE_CVE_ID.search(page_text)
              cve_id = cve_match.group(0) if cve_match else os.path.basename(url).upper()
              expected_match = RE_CVE_ID.search(url.upper())
              expected_cve = expected_match.group(0) if expected_match else None

              # Guardrail: do not parse generic pages as CVE detail pages.
              if expected_cve and expected_cve not in page_text:
                  raise ValueError(f"Expected {expected_cve} not present in response body")

              detail_container = None
              for tag in soup.find_all(["main", "article", "section", "div"]):
                  t = tag.get_text(" ", strip=True)
                  if not t:
                      continue
                  if expected_cve and expected_cve not in t:
                      continue
                  if "Published" in t or "CVSS" in t or "Status" in t:
                      detail_container = tag
                      break
              if detail_container is None:
                  detail_container = soup

              detail_text = detail_container.get_text("\n", strip=True)
              summary = pick_best_summary(detail_container, cve_id)

              refs = []
              for a in detail_container.find_all("a", href=True):
                  href = a["href"].strip()
                  if href.startswith("http") and "akaoma.com" not in href:
                      refs.append(href)

              text_inline = detail_text.replace("\n", " ")
              cvss_vector = None
              m_vec = RE_CVSS_VECTOR.search(text_inline)
              if m_vec:
                  cvss_vector = m_vec.group(1)

              cvss_score = None
              m_score = RE_CVSS_SCORE.search(text_inline)
              if m_score:
                  try:
                      cvss_score = float(m_score.group(1))
                  except ValueError:
                      cvss_score = None

              cwe_related = set()
              capec_related = set()
              vendor_related = set()
              for a in detail_container.find_all("a", href=True):
                  nurl = normalize_url(a["href"])
                  if not nurl:
                      continue
                  path = urlparse(nurl).path
                  if path.startswith("/cwe"):
                      cwe_related.add(nurl)
                  elif path.startswith("/capec"):
                      capec_related.add(nurl)
                  elif path.startswith("/vendor"):
                      vendor_related.add(nurl)

              return {
                  "cve_id": cve_id,
                  "status": extract_labeled_value(detail_text, ["Status"]),
                  "published_at": extract_labeled_value(detail_text, ["Published", "Date Published"]),
                  "updated_at": extract_labeled_value(detail_text, ["Updated", "Last Updated", "Date Updated"]),
                  "cvss": {
                      "version": extract_labeled_value(detail_text, ["CVSS Version"]),
                      "score": cvss_score,
                      "vector": cvss_vector,
                  },
                  "summary": summary,
                  "references": sorted(set(refs)),
                  "related": {
                      "cwe_urls": sorted(cwe_related),
                      "capec_urls": sorted(capec_related),
                      "vendor_urls": sorted(vendor_related),
                  },
                  "source_url": url,
              }

          existing_cves = read_json_list(os.path.join(OUT_DIR, "cves.json"))
          cve_map = {}
          known_urls = set()
          for item in existing_cves:
              if not isinstance(item, dict):
                  continue
              cve_id = item.get("cve_id")
              source_url = normalize_url(item.get("source_url", "")) if item.get("source_url") else None
              key = cve_id or source_url
              if not key:
                  continue
              if source_url:
                  item["source_url"] = source_url
                  known_urls.add(source_url)
              cve_map[key] = item

          existing_cwe = {
              (entry.get("name", ""), normalize_url(entry.get("url", "") or ""))
              for entry in read_json_list(os.path.join(OUT_DIR, "cwe.json"))
              if isinstance(entry, dict)
          }
          existing_capec = {
              (entry.get("name", ""), normalize_url(entry.get("url", "") or ""))
              for entry in read_json_list(os.path.join(OUT_DIR, "capec.json"))
              if isinstance(entry, dict)
          }
          existing_vendor = {
              (entry.get("name", ""), normalize_url(entry.get("url", "") or ""))
              for entry in read_json_list(os.path.join(OUT_DIR, "vendor.json"))
              if isinstance(entry, dict)
          }
          existing_cwe = {(n, u) for n, u in existing_cwe if u}
          existing_capec = {(n, u) for n, u in existing_capec if u}
          existing_vendor = {(n, u) for n, u in existing_vendor if u}

          priority_urls = set()
          discovered_cve_urls = set()
          discovered_cwe = set(existing_cwe)
          discovered_capec = set(existing_capec)
          discovered_vendor = set(existing_vendor)
          crawl_errors = []

          for seed in PRIORITY_SEEDS:
              try:
                  r = session.get(seed, timeout=REQUEST_TIMEOUT_SECONDS)
                  r.raise_for_status()
                  soup = BeautifulSoup(r.text, "html.parser")
                  cve_urls, _, cwe_links, capec_links, vendor_links = extract_links(soup)
                  priority_urls.update(cve_urls)
                  discovered_cve_urls.update(cve_urls)
                  discovered_cwe.update(cwe_links)
                  discovered_capec.update(capec_links)
                  discovered_vendor.update(vendor_links)
              except Exception as exc:
                  crawl_errors.append({"url": seed, "error": str(exc)})

          visited_pages = set()
          queue = deque(sorted(set(DISCOVERY_SEEDS)))

          while queue and len(visited_pages) < MAX_DISCOVERY_PAGES:
              current = queue.popleft()
              if current in visited_pages:
                  continue
              visited_pages.add(current)

              try:
                  r = session.get(current, timeout=REQUEST_TIMEOUT_SECONDS)
                  r.raise_for_status()
                  soup = BeautifulSoup(r.text, "html.parser")
              except Exception as exc:
                  crawl_errors.append({"url": current, "error": str(exc)})
                  continue

              cve_urls, crawl_links, cwe_links, capec_links, vendor_links = extract_links(soup)
              discovered_cve_urls.update(cve_urls)
              discovered_cwe.update(cwe_links)
              discovered_capec.update(capec_links)
              discovered_vendor.update(vendor_links)

              for link in sorted(crawl_links):
                  if link not in visited_pages:
                      queue.append(link)

          new_urls = sorted(discovered_cve_urls - known_urls)
          priority_existing_urls = sorted(priority_urls & known_urls)
          suspicious_urls = []
          for item in cve_map.values():
              if not isinstance(item, dict):
                  continue
              source_url = normalize_url(item.get("source_url", "") or "")
              if not source_url:
                  continue
              summary = clean_text(item.get("summary", "") or "")
              updated_at = clean_text(item.get("updated_at", "") or "")
              bad_summary = "CVE Threat Dashboard" in summary or summary.startswith("CVE Threat Dashboard")
              bad_updated = updated_at in {"", "ðŸ•"} or re.fullmatch(r"[\W_]+", updated_at or "") is not None
              if bad_summary or bad_updated:
                  suspicious_urls.append(source_url)
          suspicious_urls = sorted(set(suspicious_urls))

          fetch_queue = []
          seen_fetch = set()

          for url in sorted(priority_urls):
              if url not in seen_fetch:
                  fetch_queue.append(url)
                  seen_fetch.add(url)

          for url in suspicious_urls:
              if url not in seen_fetch:
                  fetch_queue.append(url)
                  seen_fetch.add(url)

          for url in new_urls:
              if url not in seen_fetch:
                  fetch_queue.append(url)
                  seen_fetch.add(url)

          fetch_queue = fetch_queue[:MAX_DETAIL_FETCH_PER_RUN]

          detail_errors = []
          fetched_count = 0
          for url in fetch_queue:
              try:
                  detail = parse_detail(url)
                  key = detail.get("cve_id") or detail.get("source_url")
                  if key:
                      cve_map[key] = detail
                      fetched_count += 1
              except Exception as exc:
                  detail_errors.append({"url": url, "error": str(exc)})

          merged_cves = sorted(
              cve_map.values(),
              key=lambda x: (x.get("cve_id") or "", x.get("source_url") or ""),
          )

          cwe_json = [{"name": name, "url": url} for name, url in sorted(discovered_cwe, key=lambda x: (x[0], x[1]))]
          capec_json = [{"name": name, "url": url} for name, url in sorted(discovered_capec, key=lambda x: (x[0], x[1]))]
          vendor_json = [{"name": name, "url": url} for name, url in sorted(discovered_vendor, key=lambda x: (x[0], x[1]))]

          with open(os.path.join(OUT_DIR, "cves.json"), "w", encoding="utf-8") as f:
              json.dump(merged_cves, f, ensure_ascii=False, indent=2, sort_keys=True)

          with open(os.path.join(OUT_DIR, "cwe.json"), "w", encoding="utf-8") as f:
              json.dump(cwe_json, f, ensure_ascii=False, indent=2, sort_keys=True)

          with open(os.path.join(OUT_DIR, "capec.json"), "w", encoding="utf-8") as f:
              json.dump(capec_json, f, ensure_ascii=False, indent=2, sort_keys=True)

          with open(os.path.join(OUT_DIR, "vendor.json"), "w", encoding="utf-8") as f:
              json.dump(vendor_json, f, ensure_ascii=False, indent=2, sort_keys=True)

          with open(os.path.join(OUT_DIR, "crawl-errors.json"), "w", encoding="utf-8") as f:
              json.dump(sorted(crawl_errors, key=lambda x: x["url"]), f, ensure_ascii=False, indent=2, sort_keys=True)

          with open(os.path.join(OUT_DIR, "detail-errors.json"), "w", encoding="utf-8") as f:
              json.dump(sorted(detail_errors, key=lambda x: x["url"]), f, ensure_ascii=False, indent=2, sort_keys=True)

          with open(os.path.join(OUT_DIR, "index.json"), "w", encoding="utf-8") as f:
              json.dump(
                  {
                      "mode": "incremental",
                      "seed_urls": sorted(DISCOVERY_SEEDS),
                      "limits": {
                          "max_discovery_pages": MAX_DISCOVERY_PAGES,
                          "max_detail_fetch_per_run": MAX_DETAIL_FETCH_PER_RUN,
                          "request_timeout_seconds": REQUEST_TIMEOUT_SECONDS,
                      },
                      "totals": {
                          "existing_records_before": len(existing_cves),
                          "merged_records_after": len(merged_cves),
                          "priority_urls_discovered": len(priority_urls),
                          "known_urls_before": len(known_urls),
                          "new_urls_discovered": len(new_urls),
                          "priority_existing_urls": len(priority_existing_urls),
                          "suspicious_urls_repair_candidate": len(suspicious_urls),
                          "detail_urls_fetched_this_run": fetched_count,
                          "detail_urls_attempted_this_run": len(fetch_queue),
                          "pages_crawled": len(visited_pages),
                          "crawl_errors": len(crawl_errors),
                          "detail_errors": len(detail_errors),
                          "cwe_links_total": len(cwe_json),
                          "capec_links_total": len(capec_json),
                          "vendor_links_total": len(vendor_json),
                      },
                      "detail_fetch_queue_sample": fetch_queue[:200],
                  },
                  f,
                  ensure_ascii=False,
                  indent=2,
                  sort_keys=True,
              )
          PY

      - name: Commit and push if changed
        run: |
          git config user.name "github-actions"
          git config user.email "actions@github.com"
          git add cve-curated/
          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi

          git commit -m "Auto-update cve-curated data (incremental)"

          BRANCH="${GITHUB_REF_NAME:-main}"
          for attempt in 1 2 3; do
            git pull --rebase origin "$BRANCH"
            if git push origin "HEAD:$BRANCH"; then
              echo "Push succeeded on attempt $attempt"
              exit 0
            fi
            echo "Push failed on attempt $attempt, retrying..."
            sleep $((attempt * 5))
          done

          echo "Push failed after 3 attempts."
          exit 1
